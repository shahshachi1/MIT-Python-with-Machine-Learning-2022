{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Data-Science-and-Data-Analytics-Courses/MITx---Machine-Learning-with-Python-From-Linear-Models-to-Deep-Learning-Jun-11-2019/blob/master/Unit%202%20Nonlinear%20Classification%2C%20Linear%20regression%2C%20Collaborative%20Filtering%20(2%20weeks)/Exercises/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKf749g6256",
        "colab_type": "text"
      },
      "source": [
        "# Linear Regression\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDXChezQ-Rf2",
        "colab_type": "text"
      },
      "source": [
        "## Packages and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOi005Iv-bgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt-03reH8J7v",
        "colab_type": "text"
      },
      "source": [
        "## Compute Hinge Loss\n",
        "The empirical risk  Rn  is defined as\n",
        "\n",
        "$$\n",
        "R_ n(\\theta ) = \\frac{1}{n} \\sum _{t=1}^{n} \\text {Loss}(y^{(t)} - \\theta \\cdot x^{(t)})\n",
        "$$\n",
        "\n",
        "where  (x(t),y(t))  is the  t th training example (and there are  n  in total), and  Loss  is some loss function, such as hinge loss.\n",
        "\n",
        "Recall from a previous lecture that the definition of hinge loss:\n",
        "\n",
        "$$\n",
        "\\text {Loss}_ h(z) = \\begin{cases}  0 & \\text {if } z \\geq 1 \\\\ 1 -z, & \\text { otherwise} \\end{cases}.\n",
        "$$\n",
        "\n",
        "In this problem, we calculate the empirical risk with hinge loss when given specific  Î¸  and  {(x(t),y(t))}t=1,...,n .  \n",
        "\n",
        "Compute the value of\n",
        "\n",
        "$$\n",
        "R_ n(\\theta ) = \\frac{1}{4} \\sum _{t=1}^{4} \\text {Loss}_ h(y^{(t)} - \\theta \\cdot x^{(t)}).\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XT1RK1b7frPk",
        "colab_type": "code",
        "outputId": "66b98028-96d4-48da-a6ed-f0c107dab793",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def hinge_loss(x, y, theta, theta0=0):\n",
        "  x = np.asarray(x)\n",
        "  theta = np.asarray(theta)\n",
        "  \n",
        "  n, d = x.shape\n",
        "  z = y - (x.dot(theta)+theta0)\n",
        "  losses = np.where(z>=1, 0, 1-z)\n",
        "  loss = 1/n * losses.sum()\n",
        "  \n",
        "  return loss\n",
        "\n",
        "x = np.array([[1, 0, 1], \n",
        "              [1, 1, 1],\n",
        "              [1, 1, -1],\n",
        "              [-1, 1, 1]])\n",
        "y= [2, 2.7, -0.7, 2]\n",
        "theta = [0,1,2]\n",
        "hinge_loss(x, y, theta)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TUz05SYFnwU",
        "colab_type": "text"
      },
      "source": [
        "## Compute Squared Error Loss\n",
        "Now, we will calculate the empirical risk with the squared error loss. Remember that the squared error loss is given by  $\\displaystyle \\text {Loss}(z) = \\frac{z^2}{2}.$  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZLWyeHVFYE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "337effdf-478a-4238-eec0-252a696f0108"
      },
      "source": [
        "def squared_error_loss(x, y, theta, theta0=0):\n",
        "  x = np.asarray(x)\n",
        "  theta = np.asarray(theta)\n",
        "  \n",
        "  n, d = x.shape\n",
        "  z = y - (x.dot(theta)+theta0)\n",
        "  loss = 1/n * np.sum(1/2 * z**2)\n",
        "  \n",
        "  return loss\n",
        "\n",
        "x = np.array([[1, 0, 1], \n",
        "              [1, 1, 1],\n",
        "              [1, 1, -1],\n",
        "              [-1, 1, 1]])\n",
        "y= [2, 2.7, -0.7, 2]\n",
        "theta = [0,1,2]\n",
        "squared_error_loss(x, y, theta)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    }
  ]
}